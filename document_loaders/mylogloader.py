import streamlit as st
import pandas as pd
import os

class Preprocess():
    def __int__(self, ):
        # self.chunk = KnowledgeChunk()
        self.documents = []
        self.df = {}
        self.idf = {}
        self.average_doc_len = 0

    def get_bm25_model(self, contents):
        # BM25_store
        _ = [self.add_document(doc) for doc in contents]
        self.calculate_idf()

        # Store in session

    # é€šç”¨å¤„ç†
    def process_documents(self, uploaded_files):
        # if st.session_state.documents_loaded:
        #     return

        # # Text splitting
        # print(documents)
        # exit()
        documents, doc_name = self.file_parse(uploaded_files)  # è§£æè‡ªå®šä¹‰ææ–™
        # print("documents", documents)
        # print("doc_name", doc_name)
        for doc in documents:
            text_contents = self.split_sentences(doc[0].page_content)  # åˆ‡ç‰‡
            st.session_state.documents += text_contents
        # print(st.session_state.documents)

        self.get_bm25_model(st.session_state.documents)
        st.session_state.retrieval_pipeline = {"texts": st.session_state.documents}
        st.write(f"ğŸ”— Total Chunk: {len(st.session_state.documents)}")

        st.session_state.documents_loaded = True
        # st.session_state.processing = False
        return documents, doc_name

    def file_parse(self, uploaded_files):
        files_bag, file_name = [], []
        # st.session_state.processing = True
        # documents = []

        # Create temp directory
        if not os.path.exists("temp"):
            os.makedirs("temp")

        # Process files
        for file in uploaded_files:
            try:
                file_path = os.path.join("temp", file.name)
                with open(file_path, "wb") as f:
                    f.write(file.getbuffer())


                documents = loader.load()
                files_bag.append(documents)
                file_name.append(file.name)
                os.remove(file_path)

            except Exception as e:
                st.error(f"Error processing {file.name}: {str(e)}")
                return
        return files_bag, file_name



    # æ—¥å¿—ææ–™å¤„ç†
    def log_parse(self, uploaded_files):
        # st.session_state.processing = True
        # documents = []

        # Create temp directory
        if not os.path.exists("temp"):
            os.makedirs("temp")

        # Process files
        for file in uploaded_files:
            try:
                file_path = os.path.join("temp", file.name)
                with open(file_path, "wb") as f:
                    f.write(file.getbuffer())

                if file.name.endswith(".xlsx") or file.name.endswith(".xls"):
                    documents = pd.read_excel(file_path)
                elif file.name.endswith(".csv"):
                    documents = pd.read_csv(file_path)
                else:
                    continue

                os.remove(file_path)
            except Exception as e:
                st.error(f"Error processing {file.name}: {str(e)}")
                return

        if "æ“ä½œæ—¥å¿—" in uploaded_files[0].name:  # ä¸»æœºæ“ä½œæ—¥å¿—
            account_counts, daily_counts, operation_counts, account_daily_counts, error_login_counts, error_login_daily_counts = OperationLog(
                documents)
            return "æ“ä½œæ—¥å¿—", [account_counts, daily_counts, operation_counts, account_daily_counts,
                                error_login_counts, error_login_daily_counts]
        elif "ç™»å½•æ—¥å¿—" in uploaded_files[0].name:  # ä¸»æœºç™»å½•æ—¥å¿—
            login_counts, daily_counts, pivot_table, error_login_counts, error_pivot_table = EnterLog(documents)
            return "ç™»å½•æ—¥å¿—", [login_counts, daily_counts, pivot_table, error_login_counts, error_pivot_table]
        elif "ä½¿ç”¨æ—¥å¿—" in uploaded_files[0].name:  # ä¸»æœºç™»å½•æ—¥å¿—
            login_id_counts, operation_counts, daily_counts, login_operation_counts, login_daily_counts, error_login_counts, error_login_daily_counts = UseLog(
                documents)
            return "ä½¿ç”¨æ—¥å¿—", [login_id_counts, operation_counts, daily_counts, login_operation_counts,
                                login_daily_counts, error_login_counts, error_login_daily_counts]

        # def process_log(self, uploaded_files):
        #     # if st.session_state.documents_loaded:
        #     #     return
        #
        #     # # Text splitting
        #     # print(documents)
        #     # exit()
        #     log_files = self.log_parse(uploaded_files)
        #     if "æ“ä½œæ—¥å¿—" in  uploaded_files[0].name:#ä¸»æœºæ“ä½œæ—¥å¿—
        #         account_counts, daily_counts, operation_counts, account_daily_counts =OperationLog(log_files)

        # st.session_state.processing = False


if __name__ == '__main__':
    process = Preprocess()
    # chunk = process.split_recursive(
    #     "è¿‘æ—¥ï¼Œä¸­å…±ä¸­å¤®åŠå…¬å…å°å‘äº†ã€Šå…¨å›½å…šå‘˜æ•™è‚²åŸ¹è®­å·¥ä½œè§„åˆ’ï¼ˆ2024â€”2028å¹´ï¼‰ã€‹ï¼Œå¹¶å‘å‡ºé€šçŸ¥ï¼Œè¦æ±‚å„åœ°åŒºå„éƒ¨é—¨ç»“åˆå®é™…è®¤çœŸè´¯å½»è½å®ã€‚ã€Šå…¨å›½å…šå‘˜æ•™è‚²åŸ¹è®­å·¥ä½œè§„åˆ’ï¼ˆ2024â€”2028å¹´ï¼‰ã€‹å…¨æ–‡å¦‚ä¸‹ã€‚ä¸ºåŠ å¼ºå’Œæ”¹è¿›å…šå‘˜æ•™è‚²åŸ¹è®­å·¥ä½œï¼Œé”»é€ è¿‡ç¡¬å…šå‘˜é˜Ÿä¼ï¼Œä¸æ–­å¢å¼ºå…šçš„åˆ›é€ åŠ›ã€å‡èšåŠ›ã€æˆ˜æ–—åŠ›ï¼Œæ ¹æ®ã€Šä¸­å›½å…±äº§å…šç« ç¨‹ã€‹å’Œã€Šä¸­å›½å…±äº§å…šå…šå‘˜æ•™è‚²ç®¡ç†å·¥ä½œæ¡ä¾‹ã€‹ç­‰å…šå†…æ³•è§„ï¼Œåˆ¶å®šæœ¬è§„åˆ’ã€‚ä¸€ã€æ€»ä½“è¦æ±‚åšæŒä»¥ä¹ è¿‘å¹³æ–°æ—¶ä»£ä¸­å›½ç‰¹è‰²ç¤¾ä¼šä¸»ä¹‰æ€æƒ³ä¸ºæŒ‡å¯¼ï¼Œæ·±å…¥è´¯å½»å…šçš„äºŒåå¤§å’ŒäºŒåå±ŠäºŒä¸­ã€ä¸‰ä¸­å…¨ä¼šç²¾ç¥ï¼Œå…¨é¢è´¯å½»ä¹ è¿‘å¹³æ€»ä¹¦è®°å…³äºå…šçš„å»ºè®¾çš„é‡è¦æ€æƒ³ã€å…³äºå…šçš„è‡ªæˆ‘é©å‘½çš„é‡è¦æ€æƒ³ï¼Œæ·±å…¥è½å®æ–°æ—¶ä»£å…šçš„å»ºè®¾æ€»è¦æ±‚å’Œæ–°æ—¶ä»£å…šçš„ç»„ç»‡è·¯çº¿ï¼Œä»¥ç”¨å…šçš„åˆ›æ–°ç†è®ºæ­¦è£…å…¨å…šä¸ºé¦–è¦æ”¿æ²»ä»»åŠ¡ï¼Œä»¥å¢å¼ºå…šæ€§ã€æé«˜ç´ è´¨ã€å‘æŒ¥ä½œç”¨ä¸ºé‡ç‚¹ï¼ŒåšæŒæ”¿æ²»å¼•é¢†ã€åˆ†ç±»æŒ‡å¯¼ã€å®ˆæ­£åˆ›æ–°ã€æœåŠ¡å¤§å±€ï¼Œæ•™è‚²å¼•å¯¼å…¨ä½“å…šå‘˜æ·±åˆ»é¢†æ‚Ÿâ€œä¸¤ä¸ªç¡®ç«‹â€çš„å†³å®šæ€§æ„ä¹‰ï¼Œå¢å¼ºâ€œå››ä¸ªæ„è¯†â€ã€åšå®šâ€œå››ä¸ªè‡ªä¿¡â€ã€åšåˆ°â€œä¸¤ä¸ªç»´æŠ¤â€ï¼Œå¼€æ‹“è¿›å–ã€å¹²äº‹åˆ›ä¸šï¼Œä¸ºä»¥ä¸­å›½å¼ç°ä»£åŒ–å…¨é¢æ¨è¿›å¼ºå›½å»ºè®¾ã€æ°‘æ—å¤å…´ä¼Ÿä¸šæä¾›æ€æƒ³æ”¿æ²»ä¿è¯å’Œèƒ½åŠ›æ”¯æ’‘ã€‚",
    #     chunk_size=20)
    # print(chunk)
    # _ = [process.add_document(doc) for doc in chunk]
    # # process.add_document(chunk)
    # process.calculate_idf()
    # print(process.search("è¿‘æ—¥ï¼Œä¸­å…±ä¸­å¤®åŠå…¬å…å°å‘äº†ä»€ä¹ˆ"))
    loader = Docx2txtLoader(r"D:\work\ä¸­å°\ä¸­å°å®‰å…¨è¿è¥å·¥å…·\test\ä¸­å°æ£€æŸ¥æ–‡æ¡£éœ€æ±‚æ¸…å•.docx")
    documents = loader.load()
    print(documents)
