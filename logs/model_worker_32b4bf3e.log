2024-10-22 11:51:16 | INFO | model_worker | Loading the model ['测试模型1-14B'] on worker 32b4bf3e ...
2024-10-22 11:51:16 | ERROR | stderr | Loading checkpoint shards:   0%|                                                                                                                                  | 0/8 [00:00<?, ?it/s]
2024-10-22 11:51:17 | ERROR | stderr | Loading checkpoint shards:  12%|███████████████▎                                                                                                          | 1/8 [00:01<00:08,  1.17s/it]
2024-10-22 11:51:18 | ERROR | stderr | Loading checkpoint shards:  25%|██████████████████████████████▌                                                                                           | 2/8 [00:02<00:06,  1.08s/it]
2024-10-22 11:51:19 | ERROR | stderr | Loading checkpoint shards:  38%|█████████████████████████████████████████████▊                                                                            | 3/8 [00:03<00:05,  1.04s/it]
2024-10-22 11:51:20 | ERROR | stderr | Loading checkpoint shards:  50%|█████████████████████████████████████████████████████████████                                                             | 4/8 [00:04<00:04,  1.10s/it]
2024-10-22 11:51:21 | ERROR | stderr | Loading checkpoint shards:  62%|████████████████████████████████████████████████████████████████████████████▎                                             | 5/8 [00:05<00:03,  1.07s/it]
2024-10-22 11:51:22 | ERROR | stderr | Loading checkpoint shards:  62%|████████████████████████████████████████████████████████████████████████████▎                                             | 5/8 [00:05<00:03,  1.19s/it]
2024-10-22 11:51:22 | ERROR | stderr | 
2024-10-22 11:51:22 | ERROR | stderr | Process model_worker - 测试模型1-14B:
2024-10-22 11:51:22 | ERROR | stderr | Traceback (most recent call last):
2024-10-22 11:51:22 | ERROR | stderr |   File "/home/ding/anaconda3/envs/security/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
2024-10-22 11:51:22 | ERROR | stderr |     self.run()
2024-10-22 11:51:22 | ERROR | stderr |   File "/home/ding/anaconda3/envs/security/lib/python3.11/multiprocessing/process.py", line 108, in run
2024-10-22 11:51:22 | ERROR | stderr |     self._target(*self._args, **self._kwargs)
2024-10-22 11:51:22 | ERROR | stderr |   File "/home/ding/LLM/startup.py", line 389, in run_model_worker
2024-10-22 11:51:22 | ERROR | stderr |     app = create_model_worker_app(log_level=log_level, **kwargs)
2024-10-22 11:51:22 | ERROR | stderr |           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-10-22 11:51:22 | ERROR | stderr |   File "/home/ding/LLM/startup.py", line 217, in create_model_worker_app
2024-10-22 11:51:22 | ERROR | stderr |     worker = ModelWorker(
2024-10-22 11:51:22 | ERROR | stderr |              ^^^^^^^^^^^^
2024-10-22 11:51:22 | ERROR | stderr |   File "/home/ding/anaconda3/envs/security/lib/python3.11/site-packages/fastchat/serve/model_worker.py", line 77, in __init__
2024-10-22 11:51:22 | ERROR | stderr |     self.model, self.tokenizer = load_model(
2024-10-22 11:51:22 | ERROR | stderr |                                  ^^^^^^^^^^^
2024-10-22 11:51:22 | ERROR | stderr |   File "/home/ding/anaconda3/envs/security/lib/python3.11/site-packages/fastchat/model/model_adapter.py", line 348, in load_model
2024-10-22 11:51:22 | ERROR | stderr |     model, tokenizer = adapter.load_model(model_path, kwargs)
2024-10-22 11:51:22 | ERROR | stderr |                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-10-22 11:51:22 | ERROR | stderr |   File "/home/ding/anaconda3/envs/security/lib/python3.11/site-packages/fastchat/model/model_adapter.py", line 1685, in load_model
2024-10-22 11:51:22 | ERROR | stderr |     model = AutoModelForCausalLM.from_pretrained(
2024-10-22 11:51:22 | ERROR | stderr |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-10-22 11:51:22 | ERROR | stderr |   File "/home/ding/anaconda3/envs/security/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 566, in from_pretrained
2024-10-22 11:51:22 | ERROR | stderr |     return model_class.from_pretrained(
2024-10-22 11:51:22 | ERROR | stderr |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-10-22 11:51:22 | ERROR | stderr |   File "/home/ding/anaconda3/envs/security/lib/python3.11/site-packages/transformers/modeling_utils.py", line 3850, in from_pretrained
2024-10-22 11:51:22 | ERROR | stderr |     ) = cls._load_pretrained_model(
2024-10-22 11:51:22 | ERROR | stderr |         ^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-10-22 11:51:22 | ERROR | stderr |   File "/home/ding/anaconda3/envs/security/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4284, in _load_pretrained_model
2024-10-22 11:51:22 | ERROR | stderr |     new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
2024-10-22 11:51:22 | ERROR | stderr |                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2024-10-22 11:51:22 | ERROR | stderr |   File "/home/ding/anaconda3/envs/security/lib/python3.11/site-packages/transformers/modeling_utils.py", line 805, in _load_state_dict_into_meta_model
2024-10-22 11:51:22 | ERROR | stderr |     set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
2024-10-22 11:51:22 | ERROR | stderr |   File "/home/ding/anaconda3/envs/security/lib/python3.11/site-packages/accelerate/utils/modeling.py", line 317, in set_module_tensor_to_device
2024-10-22 11:51:22 | ERROR | stderr |     new_value = value.to(device)
2024-10-22 11:51:22 | ERROR | stderr |                 ^^^^^^^^^^^^^^^^
2024-10-22 11:51:22 | ERROR | stderr | torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 50.00 MiB. GPU 1 has a total capacty of 79.14 GiB of which 3.19 MiB is free. Process 105727 has 70.08 GiB memory in use. Including non-PyTorch memory, this process has 9.03 GiB memory in use. Of the allocated memory 8.62 GiB is allocated by PyTorch, and 1.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
